{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\44787\\\\Desktop\\\\projects\\\\Potato-disease-end-to-end\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\44787\\\\Desktop\\\\projects\\\\Potato-disease-end-to-end'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    dataset_path: Path\n",
    "    model_save: Path\n",
    "    BATCH_SIZE: int\n",
    "    IMAGE_SIZE: int\n",
    "    CHANNELS: int\n",
    "    EPOCHS: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PotatoDisease.constants import PARAMS_PATH, CONFIG_PATH\n",
    "from PotatoDisease.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config=CONFIG_PATH,\n",
    "                 params=PARAMS_PATH) -> None:\n",
    "        self.config = read_yaml(config)\n",
    "        self.params = read_yaml(params)\n",
    "\n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        config = self.config.model_training\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_training_config = ModelTrainingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            dataset_path=config.dataset_path,\n",
    "            model_save=config.model_save,\n",
    "            BATCH_SIZE=self.params.BATCH_SIZE,\n",
    "            IMAGE_SIZE=self.params.IMAGE_SIZE,\n",
    "            CHANNELS=self.params.CHANNELS,\n",
    "            EPOCHS=self.params.EPOCHS,\n",
    "        )\n",
    "        return model_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-18 08:19:57,081: WARNING: module_wrapper: From c:\\Users\\44787\\anaconda3\\envs\\potato-env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from PotatoDisease.logging import logger\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTraining:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def split_dataset(self,\n",
    "                      dataset,\n",
    "                      train_split=0.8,\n",
    "                      test_split=0.1,\n",
    "                      val_split=0.1,\n",
    "                      shuffle=True):\n",
    "\n",
    "        assert (train_split+test_split+val_split) == 1\n",
    "\n",
    "        dataset_size = len(dataset)\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(10000, seed=42)\n",
    "\n",
    "        train_size = int(train_split * dataset_size)\n",
    "        val_size = int(val_split * dataset_size)\n",
    "\n",
    "        train_data = dataset.take(train_size)\n",
    "        val_data = dataset.skip(train_size).take(val_size)\n",
    "        test_data = dataset.skip(train_size).skip(val_size)\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            self.config.dataset_path,\n",
    "            image_size=(self.config.IMAGE_SIZE, self.config.IMAGE_SIZE),\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "        logger.info(f'name of classes: {dataset.class_names}')\n",
    "\n",
    "        train_data, val_data, test_data = self.split_dataset(dataset)\n",
    "\n",
    "        train_data = train_data.cache().shuffle(\n",
    "            1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        val_data = val_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        test_data = test_data.cache().shuffle(\n",
    "            1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        # applay data agumentation to train dataset\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.RandomFlip(\n",
    "                \"horizontal_and_vertical\"),\n",
    "            tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "        ])\n",
    "        train_data = train_data.map(lambda x, y:\n",
    "                                    (data_augmentation(x, training=True), y)).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    def training(self):\n",
    "        resize_and_rescale = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.Resizing(\n",
    "                self.config.IMAGE_SIZE, self.config.IMAGE_SIZE),\n",
    "            tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "        ])\n",
    "\n",
    "        # create a model architecture\n",
    "        input_shape = (self.config.BATCH_SIZE, self.config.IMAGE_SIZE,\n",
    "                       self.config.IMAGE_SIZE, self.config.CHANNELS)\n",
    "        n_classes = 3\n",
    "\n",
    "        model = tf.keras.models.Sequential([\n",
    "            resize_and_rescale,\n",
    "            tf.keras.layers.Conv2D(32, kernel_size=(\n",
    "                3, 3), activation='relu', input_shape=input_shape),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64,  kernel_size=(3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64,  kernel_size=(3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(n_classes, activation='softmax'),\n",
    "        ])\n",
    "        model.build(input_shape=input_shape)\n",
    "        logger.info(model.summary())\n",
    "\n",
    "        # compile the model\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=False),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        train_data, val_data, test_data = self.prepare_dataset()\n",
    "\n",
    "        # train the model\n",
    "        history = model.fit(\n",
    "            train_data,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            validation_data=val_data,\n",
    "            verbose=1,\n",
    "            epochs=self.config.EPOCHS\n",
    "        )\n",
    "\n",
    "        # evaluate the model\n",
    "        results = model.evaluate(test_data)\n",
    "        logger.info(f\"Model evaluation: {results}\")\n",
    "\n",
    "        self.save_model(\n",
    "            self.config.model_save,\n",
    "            model\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: tf.keras.Model):\n",
    "        model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-18 08:19:58,428: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-12-18 08:19:58,431: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-12-18 08:19:58,433: INFO: common: created directory at: artifacts/model_training]\n",
      "[2023-12-18 08:19:58,699: WARNING: module_wrapper: From c:\\Users\\44787\\anaconda3\\envs\\potato-env\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "]\n",
      "[2023-12-18 08:19:58,803: WARNING: module_wrapper: From c:\\Users\\44787\\anaconda3\\envs\\potato-env\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (32, 256, 256, 3)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (32, 254, 254, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (32, 127, 127, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (32, 125, 125, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (32, 62, 62, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (32, 60, 60, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (32, 30, 30, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (32, 28, 28, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (32, 14, 14, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (32, 12, 12, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (32, 6, 6, 64)            0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (32, 4, 4, 64)            36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (32, 2, 2, 64)            0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (32, 256)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (32, 64)                  16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (32, 3)                   195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183747 (717.76 KB)\n",
      "Trainable params: 183747 (717.76 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "[2023-12-18 08:19:58,947: INFO: 3409840583: None]\n",
      "[2023-12-18 08:19:59,012: WARNING: module_wrapper: From c:\\Users\\44787\\anaconda3\\envs\\potato-env\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "]\n",
      "Found 2152 files belonging to 3 classes.\n",
      "[2023-12-18 08:19:59,305: INFO: 3409840583: name of classes: ['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']]\n",
      "Epoch 1/2\n",
      "[2023-12-18 08:19:59,987: WARNING: module_wrapper: From c:\\Users\\44787\\anaconda3\\envs\\potato-env\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "]\n",
      "[2023-12-18 08:20:00,346: WARNING: module_wrapper: From c:\\Users\\44787\\anaconda3\\envs\\potato-env\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "]\n",
      "54/54 [==============================] - 29s 488ms/step - loss: 0.9160 - accuracy: 0.4982 - val_loss: 0.8869 - val_accuracy: 0.5417\n",
      "Epoch 2/2\n",
      "54/54 [==============================] - 25s 461ms/step - loss: 0.7389 - accuracy: 0.6802 - val_loss: 0.7639 - val_accuracy: 0.5833\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 0.8144 - accuracy: 0.6055\n",
      "[2023-12-18 08:20:54,647: INFO: 3409840583: Model evaluation: [0.8143810033798218, 0.60546875]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\44787\\anaconda3\\envs\\potato-env\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_training_config = config.get_model_training_config()\n",
    "    model_training = ModelTraining(model_training_config)\n",
    "    model_training.training()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "potato-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
