{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\44787\\\\Desktop\\\\projects\\\\Potato-disease-end-to-end\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\44787\\\\Desktop\\\\projects\\\\Potato-disease-end-to-end'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    dataset_path: Path\n",
    "    model_save: Path\n",
    "    BATCH_SIZE: int\n",
    "    IMAGE_SIZE: int\n",
    "    CHANNELS: int\n",
    "    EPOCHS: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PotatoDisease.constants import PARAMS_PATH, CONFIG_PATH\n",
    "from PotatoDisease.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config=CONFIG_PATH,\n",
    "                 params=PARAMS_PATH) -> None:\n",
    "        self.config = read_yaml(config)\n",
    "        self.params = read_yaml(params)\n",
    "\n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        config = self.config.model_training\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_training_config = ModelTrainingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            dataset_path=config.dataset_path,\n",
    "            model_save=config.model_save,\n",
    "            BATCH_SIZE=self.params.BATCH_SIZE,\n",
    "            IMAGE_SIZE=self.params.IMAGE_SIZE,\n",
    "            CHANNELS=self.params.CHANNELS,\n",
    "            EPOCHS=self.params.EPOCHS,\n",
    "        )\n",
    "        return model_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-18 10:44:52,019: WARNING: module_wrapper: From c:\\Users\\44787\\anaconda3\\envs\\potato-env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from PotatoDisease.logging import logger\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://dagshub.com/fraidoon_omarzai/Potato-disease-end-to-end.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"fraidoon_omarzai\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"bc25b16bd5206328d8899cf34377f26ad71d1420\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTraining:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def split_dataset(self,\n",
    "                      dataset,\n",
    "                      train_split=0.8,\n",
    "                      test_split=0.1,\n",
    "                      val_split=0.1,\n",
    "                      shuffle=True):\n",
    "\n",
    "        assert (train_split+test_split+val_split) == 1\n",
    "\n",
    "        dataset_size = len(dataset)\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(10000, seed=42)\n",
    "\n",
    "        train_size = int(train_split * dataset_size)\n",
    "        val_size = int(val_split * dataset_size)\n",
    "\n",
    "        train_data = dataset.take(train_size)\n",
    "        val_data = dataset.skip(train_size).take(val_size)\n",
    "        test_data = dataset.skip(train_size).skip(val_size)\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            self.config.dataset_path,\n",
    "            image_size=(self.config.IMAGE_SIZE, self.config.IMAGE_SIZE),\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "        logger.info(f'name of classes: {dataset.class_names}')\n",
    "\n",
    "        train_data, val_data, test_data = self.split_dataset(dataset)\n",
    "\n",
    "        train_data = train_data.cache().shuffle(\n",
    "            1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        val_data = val_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        test_data = test_data.cache().shuffle(\n",
    "            1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        # applay data agumentation to train dataset\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.RandomFlip(\n",
    "                \"horizontal_and_vertical\"),\n",
    "            tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "        ])\n",
    "        train_data = train_data.map(lambda x, y:\n",
    "                                    (data_augmentation(x, training=True), y)).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    def training(self):\n",
    "        resize_and_rescale = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.Resizing(\n",
    "                self.config.IMAGE_SIZE, self.config.IMAGE_SIZE),\n",
    "            tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "        ])\n",
    "\n",
    "        # create a model architecture\n",
    "        input_shape = (self.config.BATCH_SIZE, self.config.IMAGE_SIZE,\n",
    "                       self.config.IMAGE_SIZE, self.config.CHANNELS)\n",
    "        n_classes = 3\n",
    "\n",
    "        \n",
    "        model = tf.keras.models.Sequential([\n",
    "            resize_and_rescale,\n",
    "            tf.keras.layers.Conv2D(32, kernel_size=(\n",
    "                3, 3), activation='relu', input_shape=input_shape),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64,  kernel_size=(3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64,  kernel_size=(3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(n_classes, activation='softmax'),\n",
    "        ])\n",
    "        model.build(input_shape=input_shape)\n",
    "        logger.info(model.summary())\n",
    "\n",
    "        # compile the model\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=False),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        train_data, val_data, test_data = self.prepare_dataset()\n",
    "\n",
    "\n",
    "        mlflow.set_registry_uri('https://dagshub.com/fraidoon_omarzai/Potato-disease-end-to-end.mlflow')\n",
    "        \n",
    "        with mlflow.start_run(run_name=\"Tensorflow Model\") as mlops_run:\n",
    "        \n",
    "            # train the model\n",
    "            model.fit(\n",
    "                train_data,\n",
    "                batch_size=self.config.BATCH_SIZE,\n",
    "                validation_data=val_data,\n",
    "                verbose=1,\n",
    "                epochs=self.config.EPOCHS\n",
    "            )\n",
    "            \n",
    "            # evaluate the model\n",
    "            results = model.evaluate(test_data)\n",
    "            logger.info(f\"Model evaluation: {results}\")\n",
    "            \n",
    "            mlflow.log_param('batch size', self.config.BATCH_SIZE)\n",
    "            mlflow.log_param('epochs', self.config.EPOCHS)\n",
    "            \n",
    "            mlflow.log_metric('accuracy', results[0])\n",
    "            \n",
    "            tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "            if tracking_url_type_store != \"file\":\n",
    "\n",
    "                # Register the model\n",
    "                # There are other ways to use the Model Registry, which depends on the use case,\n",
    "                # please refer to the doc for more information:\n",
    "                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "                mlflow.tensorflow.log_model(model, \"model\", registered_model_name=\"TFModel\")\n",
    "            else:\n",
    "                mlflow.tensorflow.log_model(model, \"model\")\n",
    "        \n",
    "\n",
    "    #     self.save_model(\n",
    "    #         self.config.model_save,\n",
    "    #         model\n",
    "    #     )\n",
    "\n",
    "    # @staticmethod\n",
    "    # def save_model(path: Path, model: tf.keras.Model):\n",
    "    #     model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-18 10:44:58,284: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-12-18 10:44:58,292: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-12-18 10:44:58,295: INFO: common: created directory at: artifacts/model_training]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-18 10:44:58,746: WARNING: module_wrapper: From c:\\Users\\44787\\anaconda3\\envs\\potato-env\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "]\n",
      "[2023-12-18 10:44:58,881: WARNING: module_wrapper: From c:\\Users\\44787\\anaconda3\\envs\\potato-env\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (32, 256, 256, 3)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (32, 254, 254, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (32, 127, 127, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (32, 125, 125, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (32, 62, 62, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (32, 60, 60, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (32, 30, 30, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (32, 28, 28, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (32, 14, 14, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (32, 12, 12, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (32, 6, 6, 64)            0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (32, 4, 4, 64)            36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (32, 2, 2, 64)            0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (32, 256)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (32, 64)                  16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (32, 3)                   195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183747 (717.76 KB)\n",
      "Trainable params: 183747 (717.76 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "[2023-12-18 10:44:59,049: INFO: 2964488373: None]\n",
      "[2023-12-18 10:44:59,120: WARNING: module_wrapper: From c:\\Users\\44787\\anaconda3\\envs\\potato-env\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "]\n",
      "Found 2152 files belonging to 3 classes.\n",
      "[2023-12-18 10:44:59,533: INFO: 2964488373: name of classes: ['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']]\n",
      "Epoch 1/2\n",
      "[2023-12-18 10:45:01,483: WARNING: module_wrapper: From c:\\Users\\44787\\anaconda3\\envs\\potato-env\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "]\n",
      "[2023-12-18 10:45:01,803: WARNING: module_wrapper: From c:\\Users\\44787\\anaconda3\\envs\\potato-env\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "]\n",
      "54/54 [==============================] - 33s 538ms/step - loss: 0.8701 - accuracy: 0.5088 - val_loss: 0.6870 - val_accuracy: 0.6771\n",
      "Epoch 2/2\n",
      "54/54 [==============================] - 25s 456ms/step - loss: 0.5808 - accuracy: 0.7488 - val_loss: 0.4105 - val_accuracy: 0.7396\n",
      "8/8 [==============================] - 1s 96ms/step - loss: 0.3866 - accuracy: 0.8242\n",
      "[2023-12-18 10:46:00,596: INFO: 2964488373: Model evaluation: [0.38662970066070557, 0.82421875]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/18 10:46:01 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-18 10:46:02,636: INFO: builder_impl: Assets written to: C:\\Users\\44787\\AppData\\Local\\Temp\\tmpwtelyutx\\model\\data\\model\\assets]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\44787\\anaconda3\\envs\\potato-env\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'TFModel'.\n",
      "2023/12/18 10:46:27 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: TFModel, version 1\n",
      "Created version '1' of model 'TFModel'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_training_config = config.get_model_training_config()\n",
    "    model_training = ModelTraining(model_training_config)\n",
    "    model_training.training()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "potato-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
